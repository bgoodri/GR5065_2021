---
title: Probability with Continuous Random Variables
author: Ben Goodrich
date: "`r format(Sys.time(), '%B %d, %Y')`"
autosize: true
header-includes:
   - \usepackage{amsmath}
   - \usepackage{color}
output:
  ioslides_presentation:
    widescreen: true
editor_options: 
  chunk_output_type: console
---
<style type="text/css">
slides > slide:not(.nobackground):after {
  content: '';
}
</style>

```{r, setup, include = FALSE}
options(width = 90)
library(knitr)
knit_hooks$set(small.mar = function(before, options, envir) {
    if (before) par(mar = c(4, 4, .1, .1), las = 1)  # smaller margin on top and right
})
```

```{r, joint_Pr, cache = TRUE, include = FALSE}
# from Week03/Slides03.Rmd
source(file.path("..", "Week03", "bowling.R"))
p <- 0.946
Upsilon <- 1:1000
joint_Pr <- matrix(NA, nrow = length(Upsilon), ncol = length(Omega), dimnames = list(Upsilon, Omega))
for (y in Upsilon) joint_Pr[y, ] <- p ^ y / (-y * log1p(-p)) * Pr(Omega, n = 10, Upsilon = y)
```


## Posterior Distribution Conditional on One Frame {.build}

> - If $x_1 = 7$ and $x_2 = 2$, what is 
  $\Pr\left(\Upsilon \mid p, n = 10, x_1 = 7, x_2 = 2\right)$?
```{r, post, cache = TRUE, small.mar = TRUE, fig.height=3, fig.width=10}
prior <- p ^ Upsilon / (-Upsilon * log1p(-p))
likelihood <- vapply(Upsilon, FUN.VALUE = double(1), FUN = function(y)
  return( Pr(x = 7, n = 10, Upsilon = y) * Pr(x = 2, n = 10 - 7, Upsilon = y) ) )
numerator <- prior * likelihood
denominator <- sum(numerator)
barplot(height = numerator / denominator, xlim = c(1, 37), names.arg = Upsilon)
```

## Bayesian Learning {.build}

> - If we use the posterior PMF, `joint_Pr[ , "7"] / sum(joint_Pr[ , "7"])`,
  as a "prior" for the second roll, what is 
  $\Pr\left(\Upsilon \mid p, n = 10 - 7, x_2 = 2\right)$?
```{r, post2, cache = TRUE, small.mar = TRUE, fig.height=3, fig.width=10}
prior <- joint_Pr[ , "7"] / sum(joint_Pr[ , "7"]) # same as Pr(Upsilon | p, n = 10, x_1 = 7)
likelihood <- vapply(Upsilon, FUN.VALUE = double(1), FUN = Pr, x = 2, n = 10 - 7)
numerator <- prior * likelihood
denominator <- sum(numerator)
barplot(height = numerator / denominator, xlim = c(1, 37), names.arg = Upsilon)
```

## Posterior Distribution Conditional on One Game {.build}

```{r, game, cache = TRUE, small.mar = TRUE, fig.height=2.75, fig.width=10, warning = FALSE}
frames <- rbind(
  `1st` = c(7, 2), `2nd` = c(7, 1), `3rd` = c(10, 0), `4th` = c(5, 3),  `5th` = c(9, 1),
  `6th` = c(6, 1), `7th` = c(8, 2), `8th` = c(4,  5), `9th` = c(7, 3), `10th` = c(8, 1) )
log_prior <- Upsilon * log(p) - log(-Upsilon * log1p(-p)) # use logs for numerical reasons
log_likelihood <- vapply(Upsilon, FUN.VALUE = double(1), FUN = function(y)
  sum(log_Pr(x = frames[ , 1], n = 10, Upsilon = y), 
      log_Pr(x = frames[ , 2], n = 10 - frames[ , 1], Upsilon = y)) )
numerator <- exp(log_prior + log_likelihood) # convert back to probability units
denominator <- sum(numerator, na.rm = TRUE)
barplot(numerator / denominator, xlim = c(1, 37), ylim = c(0, .21), names.arg = Upsilon)
```

## Probability and Cumulative Mass Functions

- $\Pr\left(\left.x\right|\boldsymbol{\theta}\right)$ is a Probability Mass Function (PMF) 
over a discrete $\Omega$ that may depend on some parameter(s) $\boldsymbol{\theta}$ and the 
Cumulative Mass Function (CMF) is 
$\Pr\left(\left.X\leq x\right|\boldsymbol{\theta}\right)=\sum\limits_{i = \min\{\Omega\} }^x\Pr\left(\left.i\right|\boldsymbol{\theta}\right)$
- In the first model for bowling without parameters, 
$\Pr\left(X\leq x\right) = \frac{ -1 + \mathcal{F}\left(x+2\right)}{- 1 + \mathcal{F}\left(n+2\right)}$
```{r}
source("bowling.R")
CMF <- function(x, n = 10) return( (-1 + F(x + 2)) / (-1 + F(n + 2)) )
round(CMF(Omega), digits = 5)
```
- How do we know this CMF corresponds to our PMF 
$\Pr\left(\left.x\right|n\right) = \frac{\mathcal{F}\left(x\right)}{- 1 + \mathcal{F}\left(n+2\right)}$?

## PMF is the Rate of Change in the CMF

```{r, echo=FALSE, fig.height=6,fig.width=9}
par(mar = c(5,4,0.5,0.5) + .1, las = 1)
cols <- rainbow(11)
x <- barplot(CMF(Omega), xlab = "Number of pins", ylab = "Probability of knocking down at most x pins", 
             col = cols, density = 0, border = TRUE)[,1]
for(i in 0:9) {
  j <- i + 1L
  points(x[j], CMF(i), col = cols[j], pch = 20)
  segments(x[j], CMF(i), x[j + 1L], CMF(j), col = cols[j], lty = 2)
}
abline(h = 1, lty = "dotted")
points(x[11], 1, col = cols[11], pch = 20)
```

## Cumulative Density Functions (CDFs) {.build}

> - Now $\Omega$ is an interval; e.g. $\Omega=\mathbb{R}$, $\Omega=\mathbb{R}_{+}$,
$\Omega=\left(a,b\right)$, etc.
> - $\Omega$ has an infinite number of points with zero width, so $\Pr\left(X = x\right) \downarrow 0$
> - $\Pr\left(X\leq x\right)$ is called the Cumulative Density Function (CDF) from $\Omega$ to 
$\left[0,1\right]$
> - No conceptual difference between a CMF and a CDF except emphasis on
whether $\Omega$ is discrete or continuous so we use 
$F\left(\left.x\right|\boldsymbol{\theta}\right)$ for both

## From CDF to a Probability Density Function (PDF)

> - $\Pr\left(a<X\leq x\right)=F\left(x \mid \boldsymbol{\theta}\right)-F\left(a \mid \boldsymbol{\theta}\right)$
as in the discrete case
> - If $x=a+h$, $\frac{F\left(x \mid \boldsymbol{\theta}\right)-F\left(a \mid \boldsymbol{\theta}\right)}{x-a}=\frac{F\left(a+h \mid \boldsymbol{\theta}\right)-F\left(a \mid \boldsymbol{\theta}\right)}{h}$ is the slope of a line segment
> - If we then let $h\downarrow0$, $\frac{F\left(a+h \mid \boldsymbol{\theta}\right)-F\left(a \mid \boldsymbol{\theta}\right)}{h}\rightarrow\frac{\partial F\left(a \mid \boldsymbol{\theta}\right)}{\partial a}\equiv f\left(x \mid \boldsymbol{\theta}\right)$
is still the RATE OF CHANGE in $F\left(x \mid \boldsymbol{\theta}\right)$ at $x$, i.e.
the slope of the CDF at $x$
> - The derivative of $F\left(x\right)$ with respect to $x$ is the Probability
Density Function (PDF) & denoted $f\left(x\right)$, which is always positive because the CDF increases
> - $f\left(x\right)$ is NOT a probability (it is a probability's slope) but is used like a PMF
> - Conversely, $F\left(x\mid\theta\right) = \int_{-\infty}^x f\left(x \mid \theta\right)dx$
  is the area under the PDF up to $x$
> - Can use WolframAlpha to take [derivatives](https://www.wolframalpha.com/input/?i=partial+derivative)
  or do (some) [definite integrals](https://www.wolframalpha.com/input/?i=definite+integral) 
  but Columbia students can [download](https://cuit.columbia.edu/content/mathematica) the full Mathematica for free. Also, you can do symbolic stuff in [Python](https://www.sympygamma.com/).

## Correspondence between Discrete & Continuous

Concept  | Discrete $X$ and $Y$  | Continuous $X$, $Y$, and $\theta$ | Comment
-- | ----- | ---- | -------
Cumulative      | $F\left(x \mid \theta\right) = \Pr\left(X \leq x \mid \theta\right)$  | $F\left(x \mid \theta\right) = \Pr\left(X \leq x \mid \theta\right)$ | Little distinction between CMF & CDF but they are probabilities 
Median | $\arg\min_x:F\left(x \mid \theta\right) \geq \frac{1}{2}$ | $F^{-1}\left(\frac{1}{2} \mid \theta\right) = x$ | $F^{-1}\left(p\right)$ is an inverse CDF
Rate of Change  | $\Pr\left(x \mid \theta \right) = \frac{F\left(x \mid \theta \right) - F\left(x - 1 \mid \theta\right)}{1}$  | $f\left(x \mid \theta\right) = \frac{\partial}{\partial x}F\left(x \mid \theta \right)$ | $f$ is not a probability
Mode | $\arg\max_x \Pr\left(x \mid \theta \right)$ | $\arg\max_x f\left(x \mid \theta\right)$ | Posterior mode is a red herring
$\mathbb{E}g\left(X \mid \theta\right)$ | $\sum_{x \in \Omega} g\left(x\right) \Pr\left(x \mid \theta\right)$ | $\int_{\Omega} g\left(x\right) f\left(x \mid \theta \right) dx$ | Mightn't exist in continuous case
Mult. Rule | $\Pr\left(x \mid \theta \right) \Pr\left(y \mid x, \theta\right)$ | $f\left(x \mid \theta\right) f\left(y \mid x,\theta\right)$ | Independence is a special case
Bayes Rule | $\frac{\Pr\left(x\right) \Pr\left(y \mid x\right)}{\Pr\left(y\right)} = \frac{\Pr\left(x\right) \Pr\left(y \mid x\right)}{\sum_{x \in \Omega} \Pr\left(x\right) \Pr\left(y \mid x\right)}$ | $\frac{f\left(\theta\right) f\left(y \mid \theta\right)}{f\left(y\right)} = \frac{f\left(\theta\right) f\left(y \mid \theta\right)}{\int_{-\infty}^\infty f\left(\theta\right) f\left(y \mid \theta\right)d\theta}$ | Integral is rarely elementary

## From Week02: Row is roll 1, Column is roll 2 {.smaller}

```{r, size='footnotesize', echo = FALSE, message = FALSE}
library(kableExtra)
library(dplyr)
options("kableExtra.html.bsTable" = TRUE)
options(scipen = 5)
# probability of knocking down x_1 pins on the first roll and x_2 on the second
Pr <- function(x, n = 10) ifelse(x > n, 0, F(x)) / (-1 + F(n + 2))
joint_Pr <- matrix(0, nrow = length(Omega), ncol = length(Omega))
rownames(joint_Pr) <- colnames(joint_Pr) <- names(Omega)
for (x_1 in Omega) {
  Pr_x_1 <- Pr(x_1, n = 10)
  for (x_2 in 0:(10 - x_1))
    joint_Pr[x_1 + 1, x_2 + 1] <- Pr_x_1 * Pr(x_2, n = 10 - x_1)
}
tmp <- as.data.frame(joint_Pr)
for (i in 1:ncol(tmp)) 
  tmp[,i] <- cell_spec(round(tmp[,i], digits = 6), "html", bold = tmp[,i] == 0,
                       color = ifelse(tmp[,i] == 0, "red", "black"))
kable(tmp, digits = 5, align = 'c', escape = FALSE) %>%
    kable_styling("striped", full_width = FALSE)
```

## Continuous Pin Bowling w/ [Generalized Fibonacci](https://en.wikipedia.org/wiki/Generalizations_of_Fibonacci_numbers#Extension_to_all_real_or_complex_numbers) {.build}

* What if $n$ continuous pins were arranged in an equilateral triangle w/ area $1$?

>- Let $\varphi = \frac{1 + \sqrt{5}}{2}$ and $f\left(x \mid n\right) = \frac{1}{c\left(n\right)} 
\frac{\left(2\varphi\right)^{x + 1} - \cos\left(\left(x + 1\right)\pi\right) \left(2\varphi\right)^{-x - 1}}{\sqrt{5}}$ where
the normalizing "constant" is $c\left(n\right) = \int_0^n \frac{\left(2\varphi\right)^{x + 1} - \cos\left(\left(x + 1\right)\pi\right) \left(2\varphi\right)^{-x - 1}}{\sqrt{5}} dx =$ 
$\left(\frac{\left(-1 + \varphi^n\right) \left(3 + \sqrt{5}\right)}{\ln\varphi} + \frac{2\left(\left(2 \varphi\right)^n \ln  \varphi - 2^n \cos\left(n \pi\right) \ln \varphi + 2^n \pi \sin\left(n\pi\right)\right)}{\left(2 \varphi\right)^n \left(\pi^2 + \left(\ln  \varphi\right)^2\right)}\right) \frac{1}{5 + \sqrt{5}}$

```{r}
integrate(f, lower = 0, upper = 10, n = 10) # f(x, n = 10) is defined by source("bowling.R")
```

>- If you knock down $e$ pins on the second roll, what is $f\left(x_1 \mid x_2 = e, n = 10\right)$?

## Continuous: Vertical is roll 1, Horizontal is roll 2

```{r, echo = FALSE, small.mar = TRUE, fig.height=5, fig.width=10}
SEQ <- seq(from = 0, to = 10, length.out = 100)
z <- sapply(SEQ, FUN = function(x1) f(x1) * f(SEQ, n = 10 - x1))
z <- z[ , ncol(z):1]
par(bg = "lightgrey", mar = c(0.4, 4, 3, 3) + .1)
image(x = SEQ, y = SEQ, z = z, col = heat.colors(length(z)),
      breaks = pmin(.Machine$double.xmax,
                    quantile(z, na.rm = TRUE,
                             probs = (0:length(z) + 1) / (length(z) + 1))),
      xlab = "", ylab = "First Roll", useRaster = TRUE, las = 1, axes = FALSE)
mtext("Second Roll", line = 2)
axis(3, at = 0:10)
axis(2, at = 0:10, labels = 10:0, las = 1)
points(x = sqrt(0.5), y = 10, col = "red", pch = 20)
abline(a = 0, b = 1, lwd = 5, col = "lightgrey")
txt <- quantile(z, probs = c(.2, .4, .6, .8, 0.9998), na.rm = TRUE)
legend("bottom", col = heat.colors(5), lty = 1, lwd = 5,
       legend = round(txt, digits = 5),
       bg = "lightgrey", title = "Probability Density")
legend("right", legend = "Impossible Region", box.lwd = NA)
arrows(x0 = exp(1), y0 = 0, y1 = exp(1), col = 4)
text(x = exp(1), y = 0, labels = "Condition", col = "blue", pos = 2)
```

## Posterior PDF of $X_1$ Given that $x_2 = e$ {.build}

```{r, small.mar = TRUE, fig.height=3.25, fig.width=10}
numerator <- function(x_1, x_2) { # f(x, n = 10) was defined by source("bowling.R")
  joint <- f(x_1, n = 10) * f(x_2, n = 10 - x_1)
  return( ifelse(is.na(joint), 0, joint) )
}
denominator <- integrate(numerator, lower = 0, upper = 10, x_2 = exp(1))$value
curve(numerator(x_1, x_2 = exp(1)) / denominator, from = 0, to = 10 - exp(1) - 5e-16, 
      xname = "x_1", ylab = "Posterior PDF")
```

## Elementary Functions & Std. Normal Distribution

- The set of [elementary functions](https://en.wikipedia.org/wiki/Elementary_function) is
  basically the set of functions you would find on a calculator, plus their inverses and compositions

> - The integral to obtain the previous $c\left(n\right)$ was difficult but still elementary
> - The PDF of the standard normal distribution (`dnorm` in R) is elementary
  $$f\left(x\right) = \frac{1}{\sqrt{2\pi}} e^{-\frac{1}{2} x^2} = \phi\left(x\right)$$
> - But the CDF of the standard normal distribution (`pnorm` in R) is not
  $$F\left(x\right) = \int_{-\infty}^x \phi\left(x\right) dx = \Phi\left(x\right)$$
  
> - The [Risch Algorithm](https://en.wikipedia.org/wiki/Risch_algorithm) tells us whether
  a function has an elementary antiderivative, and if so, what it is. But most CDFs are not
  elementary.

## CDF and PDF of the Standard Normal Distribution

```{r, fig.height=4.5, fig.width=9, small.mar = TRUE}
curve(pnorm(x), from = -4, to = 4, ylim = c(-.25, 1), ylab = "") # CDF
curve(dnorm(x), add = TRUE, col = "red",   lty = "dashed")       # PDF
curve(x   * dnorm(x), add = TRUE, col = 3, lty = "dotted")       # x weighted by PDF
curve(x^2 * dnorm(x), add = TRUE, col = 4, lty = "dotdash")      # x^2 weighted by PDF
```

## Location-Scale Transformations

> - If $Z$ is distributed standard normal & $X\left(Z\right) = \mu + \sigma Z$, 
  what's the PDF of $X$?
> - Answer: Note that $Z\left(X\right) = \frac{X - \mu}{\sigma}$. Since this is a monotonic
  transformation,
$$\Pr\left(X\leq x\right) = \Pr\left(Z\leq z\left(x\right)\right) = \Phi\left(z\left(x\right)\right) \\
\frac{\partial}{\partial{x}}\Phi\left(z\left(x\right)\right) = 
\frac{\partial \Phi\left(z\right)}{\partial{z}} \times \frac{\partial z\left(x\right)}{\partial{x}} = 
\phi\left(z\left(x\right)\right) \frac{1}{\sigma} = 
\frac{1}{\sigma\sqrt{2\pi}}e^{-\frac{1}{2}\left(\frac{x - \mu}{\sigma}\right)^2}$$
> - $\mathbb{E}X = \mu + \sigma \mathbb{E}Z = \mu$ and
$\mathbb{E}\left[\left(X - \mu\right)^2\right] = \mathbb{E}\left[\left(\sigma Z\right)^2\right] = 
\sigma^2\mathbb{E}\left[Z^2\right] = \sigma^2$
> - Thus, 
$f\left(\left.x\right|\mu,\sigma\right) = 
\frac{1}{\sigma\sqrt{2\pi}}e^{-\frac{1}{2}\left(\frac{x - \mu}{\sigma}\right)^2}$ 
is the PDF of the general normal distribution with expectation $\mu$ and standard deviation 
$\sigma \geq 0$ as parameters
> - The normal distribution is one of several in the 
  [location-scale family](https://en.wikipedia.org/wiki/Location%E2%80%93scale_family), 
  where such transformations only change the location and scale of the distribution

## Uniform Distribution {.build}

- The standard uniform distribution is defined over $\Omega = \left[0,1\right]$ and
  with PDF $f\left(x\right) = \mathbb{I}\{0 \leq x \leq 1\}$, CDF $F\left(x\right) = x$,
  and expectation
  $$\mathbb{E} X = \int_0^1 x \mathbb{I}\{0 \leq x \leq 1\} dx = \int_0^1 x dx = 
  \left.\frac{1}{2}x^2\right|_0^1 = \frac{1}{2}\left(1 - 0\right) = \frac{1}{2} = \mu$$

> - If $Z\left(X\right) = a + \left(b - a\right) X$, then $Z$ has a uniform distribution,
  over the $\Omega = \left[a,b\right]$ interval. To draw from a (by default standard)
  uniform distribution,
```{r}
runif(n = 7) # In R, pseudo-random number generating functions start with r...
```

## Inverse Cumulative Density Functions (ICDFs) {.build}

```{r, echo = FALSE, fig.width=10, fig.height=2.75}
par(mfrow = 1:2, mar = c(2, 4, 2, .1), las = 1)
curve(pnorm(x), from = -3, to = 3, ylab = "p = Std. Normal CDF")
mtext("x", side = 3, line = 1)
curve(qnorm(p), from =  0, to = 1, ylab = "x = Std. Normal ICDF", 
      n = 1001, ylim = c(-3, 3), col = 2, lty = 2, xname = "p")
mtext("p", side = 3, line = 1)
```

> - We can draw from any univariate distribution, by applying that distribution's inverse CDF
  to random draws from a standard uniform distribution
```{r}
c(set.seed(20210202), direct = rnorm(1), set.seed(20210202), composed = qnorm(runif(1)))
```


## Bivariate Normal Distribution

If $\Pr\left(\left.X \leq x \bigcap Y \leq y\right|\boldsymbol{\theta}\right) = 
F\left(\left.x,y\right|\boldsymbol{\theta}\right)$ is a biviariate CDF, then the
bivariate PDF is
$\frac{\partial^2}{\partial x \partial y}F\left(\left.x,y\right|\boldsymbol{\theta}\right)$.
This also generalizes beyond two dimensions. The PDF of the bivariate normal distribution over 
$\Omega = \mathbb{R}^2$ has five parameters:
$$f\left(\left.x,y\right|\mu_X,\mu_Y,\sigma_X,\sigma_Y,\rho\right) =\\
\frac{1}{2\pi\sigma_X\sigma_Y\sqrt{1-\rho^2}}e^{-\frac{1}{2\left(1-\rho^2\right)}
\left(\left(\frac{x - \mu_X}{\sigma_X}\right)^2 + 
\left(\frac{y - \mu_Y}{\sigma_Y}\right)^2 - 
2\rho\frac{x - \mu_X}{\sigma_X}\frac{y - \mu_Y}{\sigma_Y}\right)} = \\
\frac{1}{\sigma_X\sqrt{2\pi}}e^{-\frac{1}{2}\left(\frac{x - \mu_X}{\sigma_X}\right)^2} \times
\frac{1}{\color{blue}{\sigma_Y\sqrt{1-\rho^2}}\sqrt{2\pi}}e^{-\frac{1}{2}
\left(\frac{y - \left(\color{red}{\mu_Y + \frac{\sigma_Y}{\sigma_X}\rho\left(x-\mu_x\right)}\right)}
{\color{blue}{\sigma_Y\sqrt{1-\rho^2}}}\right)^2},$$ where the first term is a marginal normal PDF and the
second is a conditional normal PDF
w/ parameters $\color{red}{\mu = \mu_Y + \frac{\sigma_Y}{\sigma_X}\rho\left(x-\mu_X\right)}$ &
$\color{blue}{\sigma = \sigma_Y\sqrt{1-\rho^2}}$.

## Bivariate Normal PDF Visualized

<div class="columns-2">
```{r, webgl = TRUE, echo = TRUE, warning = FALSE, message = FALSE}
dbinorm <- function(x, y, 
                    mean_x = 5, 
                    mean_y = 5, 
                    sd_x = 1.5, 
                    sd_y = 1.0, 
                    rho = 0.5) {
  beta <- rho * sd_y / sd_x
  mu_yx <- mean_y + beta * (x - mean_x)
  sigma_yx <- sd_y * sqrt(1 - rho ^ 2)
  return( dnorm(x, mean_x, sd_x) * 
          dnorm(y, mu_yx, sigma_yx) )
}
library(rgl)
persp3d(dbinorm, xlim = c(0, 10), 
        ylim = c(0, 10), axes = FALSE,
        alpha = 0.75, col = rainbow,
        zlab = "density")
rglwidget()
```
</div>
