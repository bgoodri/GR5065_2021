---
title: "GR5065: Introduction"
author: "Ben Goodrich"
date: "`r format(Sys.time(), '%B %d, %Y')`"
autosize: true
output:
  ioslides_presentation:
    widescreen: true
---
<style type="text/css">
slides > slide:not(.nobackground):after {
content: '';
}
</style>

```{r, setup, include = FALSE}
library(ggplot2)
```

## Class Logistics

- TAs: Nick Anderson and Terry Zhang
- Course material is on GitHub https://github.com/bgoodri/GR5065_2021
- Closed captioning at https://www.streamtext.net/text.aspx?event=ProfGoodrich_Stats
- There is a post on CampusWire about logistics

## Bayesian Articles (from [Lynch and Bartlett 2019](https://www.annualreviews.org/doi/abs/10.1146/annurev-soc-073018-022457))

<div style="float: left; width: 70%;">
```{r, echo = FALSE, fig.width=10, fig.height=5}
knitr::include_graphics("figure.jpeg")
```
</div>
<div style="float: right; width: 30%;">
  > - Why the divergence in statistics after $1990$ compared to social sciences?
  > - Why much more in economics than other social sciences?
  > - Why increasing recently in psych?
  > - What is the matter with political science and sociology?
  > - Why is there a QMSS class on something that basically is not a QM in the SS?
</div>

## Quotes from McElreath (2020, p.3)

> - "statistics is neither mathematics nor a science, but rather a branch of engineering"

> - "Advanced courses in statistics do emphasize engineering, but most scientists never get that far. Teaching statistics this way is like teaching engineering backwards, starting with bridge building and ending with basic physics."

> - "Serious trouble begins when scholars move on to conducting innovative research, pushing the boundaries of their specialties. It's as if we got our hydraulic engineers by promoting plumbers."

> - "Why arenâ€™t the tests enough for innovative research? The classical procedures of introductory statistics tend to be inflexible and fragile. By inflexible, I mean that they have very limited ways to adapt to unique research contexts. By fragile, I mean that they fail in unpredictable ways when applied to new contexts."

## What Is GR5065 About?

* The analogue of "basic physics" for us is probability: a shared language for communicating about 
  uncertain (sometimes future) propositions
* QMSS does not offer / encourage a probability course such as 
  [GU4203](http://www.columbia.edu/cu/bulletin/uwb/#/cu/bulletin/uwb/subj/STAT/W4203-20211-001/)
* GR5065 is essentially a combination of:

    1. A full semester of probability at the master's level
    2. The substantial part of theory and methodology that was not covered in the theory
      and methodology [course](http://www.columbia.edu/cu/bulletin/uwb/subj/QMSS/G5010-20193-003/) QMSS 
      students are required to take
    3. Learning new software, called Stan, to put (1) and (2) into practice
    4. Unlearning much of what you thought you learned in other classes

## Obligatory Disclosure {.build}

* Ben is an employee of Columbia University, which has received several research grants to develop Stan
* Ben is also a manager of GG Statistics LLC, which uses Stan for business
* According to Columbia University 
  [policy](https://research.columbia.edu/content/conflict-interest-and-research), any such employee who 
  has any equity stake in, a title (such as officer or director) with, or is expected to earn at least 
  $\$5,000.00$ per year from a private company is required to disclose these facts in presentations

<div style="float: left; width: 60%;">
<video width="500" height="250" controls>
  <source src="https://video.twimg.com/ext_tw_video/999106109523742720/pu/vid/640x360/ljdUoEqXji0ES_CV.mp4?tag=3" type="video/mp4">
Your browser does not support the video tag.
</video> 
</div>
<div style="float: right; width: 40%;">
```{r, echo = FALSE, message = FALSE, fig.height=3, fig.width=4.5}
pp2 <- cranlogs::cran_downloads(package = "rstan", from = "2015-07-01", to = Sys.Date())
library(ggplot2)
ggplot(pp2,aes(x = date, y = count)) +
  geom_smooth(show.legend = FALSE, se = FALSE) +
  geom_hline(yintercept = 0) + 
  geom_vline(xintercept = as.numeric(as.Date("2018-05-20")), color = "red") +
  labs(x = 'Date', y = 'Daily downloads',
    title = 'RStan Daily Downloads from RStudio Mirror',
    caption = "Season 3, Episode 9 of Billions") +
  theme(plot.caption = element_text(color = "red", face = "italic"))
```
</div>


## R and Stan

* This course, like several QMSS courses, uses R as its primary language
* If you have neither used R nor another programming language (e.g. Python, Java, C++)
  this course will be VERY difficult because we do not teach the basics of R since
  QMSS students have already used R for at least one semester
* Homeworks have to be done using RMarkdown (we will explain this)
* Stan is another programming language, which we will not learn directly but 
  can be accessed from a variety of other [interfaces](http://mc-stan.org/users/interfaces/index.html),
  besides R which you might prefer to use for Bayesian inference after the course is finished 

## [The Simplest Impossible Problem](https://youtu.be/m4CjXk_b8zo)

[Collatz Conjecture](https://en.wikipedia.org/wiki/Collatz_conjecture): this R function should return 
`TRUE` for all positive integers:
```{r}
Collatz <- function(x) { # x must be a single (representable) positive integer
  stopifnot(length(x) == 1, is.numeric(x), is.finite(x), x > 0, x == floor(x))
  while (x != 1) {
    if (x %% 2 == 0) {   # x is currently even
      x <- x / 2
    } else {             # x is currently odd
      x <- 3 * x + 1
      if (is.infinite(x)) return(NA) # overflow so conjecture is undetermined
    }
  }
  return(TRUE)
}
```
If there were ANY positive integer where this sequence of $x$ values diverges to $\infty$
or enters a cycle not involving $1$, then the Collatz Conjecture would be false.

## Aristotelian (Classical Propositional) Logic

1. All men are mortal
2. Socrates is a man
3. Ergo, Socrates is mortal
* There are 0 interesting applications of deductive logic in the social sciences
* The closest is perhaps democratic peace "theory":
    1. No two democracies fight a war against each other
    2. Australia and Germany are democracies
    3. Ergo, Australia and German will not fight a war against each other
* Whether (1) is true depends on how you operationalize "democracy" and "war" so
  the literature has descended into debates over things like whether a country
  is a democracy or whether a conflict they were involved in is a war
  
>- A much better question is: Why are democracies much less likely to fight a war against
  each other compared to wars involving an autocracy?

## Probability as an Extension of Aristotelian Logic

* Various [people](https://arxiv.org/abs/1706.05261) have characterized probability as a 
  weaker form of logic where we are not necessarily certain whether propositions are true or false
* In R and many other programming languages, `TRUE` maps to $1$ and `FALSE` maps to $0$
  for the purpose of doing calculations. Probabilities are all the real
  numbers between $0$ and $1$, and we can proceed with (probabilistic) inference.
* An example (from last Spring):
    1. Unpopular presidents TEND to lose reelection campaigns
    2. Trump is, and is LIKELY to remain, unpopular
    3. Ergo, Trump will PROBABLY lose in the $2020$ election
* None of (1), (2), or (3) is guaranteed, although (3) seems plausible --- to some
  degree --- to the extent you accept both (1) and (2)

>- Bayesianism is a school of thought that uses probability to describe the
  degree of belief (with quantified uncertainty) that a proposition is true

## Iowa Electronic Market for 2020 Popular [Vote](https://iemweb.biz.uiowa.edu/graphs/Pres20_WTA.png)

```{r, echo = FALSE, fig.width=10, fig.height=5}
knitr::include_graphics("IEM.png")
```

## Five Sources of Uncertainty

1. Uncertainty about parameters in models
2. Uncertainty about which model is best
3. Uncertainty about what to do with the output of the (best) model(s)
4. Uncertainty about whether the software works as intended
5. Uncertainty about whether the (best) model(s) hold with other data

* Bayesians use probability to describe their uncertainty in (1) and (2)
* The Bayesian approach links with decision theory, which prescribes (3)
* The Stan software does as much as we can to mitigate (4)
* By implication, other approaches / software may refer to probability
  but fail to handle one or more of the above five items
* These include  randomization inference, frequentist inference, supervised 
  learning, and others

## Breakout Rooms: Randomization Inference

* Pairs of people will randomly be assigned to breakout rooms
* One person in each pair should use the `sample` function in R to 
  randomly select which person will be considered treated by passing in a 
  character vector of size two, i.e. `sample(c("name1", "name2"), size = 1)`
* Compute the difference in age (in integer years) between the treated
  person and the control person in the pair (which can be negative)
* Afterward, the control person in each pair should paste the age
  difference into the CampusWire chat

## Introduction to Randomization Inference

* In the 1920s, Fisher devised a method of inference for experiments
* There are `r format(choose(40, 20))` ways to assign $20$ out of $40$ people to treatment,
  each of which would generally imply a different estimate of the Average Treatment Effect
* The researcher chooses one of them AT RANDOM
* Randomization of the treatment variable creates the need for probability: Everything after that 
  randomization has a probability distribution that is conditioned on everything before.

## Breakout Rooms: Frequentist Inference

* Everyone will be randomly assigned to one of two breakout rooms
* Exactly one person in each group should use the `mean` function in R
  to compute the average age in the group, i.e. `mean(c(24, 27, 12, ...))`
* Afterward, that person should paste the group's average age in the CampusWire chat

## Introduction to Frequentist Inference

* In the 1920s, Fisher also devised a method of inference for observational data where
  the goal is to make an inference about a population parameter from a relatively small 
  RANDOM sample from that population
* For example, the British population was about $36$ million at the time and 
  the government might take a sample of $500$. There are $e \approx$ `r exp(1)`
  raised to the power of `r lchoose(36 * 10^6, 500)` ways of doing so.
* The data collector chooses one at random and gives the data to the analyst
  who estimates, for example, the population mean with the sample mean
* What about the other $-1 + e^{6088.1808245}$ OTHER ways that a sample of 
  size $500$ out of $36$ million could have been drawn to estimate a
  population mean?
* The probability distribution of the sample mean estimator over the 
  $e^{6088.1808245}$ ways to draw such a sample can be derived analytically
* In the 1930s, Neyman & Pearson devised a way of testing 2 point
  hypotheses about the population mean & popularized a confidence interval estimator
  
## Quotes from McElreath (2020, p.4&10)

- "The greatest obstacle that I encounter among students and colleagues is the tacit belief that the proper objective of statistical inference is to test null hypotheses"
    - Hypotheses are not models
    - Measurement error prevents deductive falsification
- "what researchers need is ... a set of principles for designing, building, and refining special-purpose statistical procedures. Every major branch of statistical philosophy possesses such a unified theory. But the theory is never taught in introductory ... courses. So there are benefits in rethinking statistical inference as a set of strategies, instead of a set of pre-made tools."

1. Bayesian data analysis
2. Multilevel models
3. Model comparison using information criteria
4. Graphical Causal Models / Directed Acyclic Graphs

## Bayesian Inference

* Uncertainty creates the need for probability to describe beliefs
* You have beliefs about how much the S&P500 will grow by the end of $2021$
* You express your beliefs with a probability distribution, such as a normal
  distribution with a mean of $+6\%$ and a standard deviation of $5\%$
* As more data comes during $2021$, you update your beliefs about
  where the S&P500 will be at the end of $2021$ to some new probability distribution
* Note the data are not, and need not be, a sample or an experiment for you to
  use probability distributions to describe your beliefs in a rigorous way 

## Supervised Learning

- Suppose there is a dataset of size $N$, which generally is neither a sample
  from any well-defined population nor an experiment so Fisher does not apply
- There are $\frac{N!}{n!\left(N - n\right)!}$ ways to divide this dataset
  into a training dataset of size $n$ and a testing dataset of size $N - n$
- The analyst chooses one split at random (with $\frac{n}{N} \approx 0.8$), then
    * chooses parameters to minimize some loss function in the training data
    * uses them to predict the outcome in the testing data
    * compares the predictive accuracy to other models
- What about the other $-1 + \frac{N!}{n!\left(N - n\right)!}$ OTHER ways to 
  divide a dataset into training and testing that constitute a 
  probability distribution over a measure of predictive accuracy? Supervised learners 
  often do not bother.
- Most (but not all) supervised learning ignores all forms of uncertainty

## Perspectives on Quantitative Methodology

What is the paradigm?      | What is fixed?                  | What is random?      | What is averaged over?    | What is the conclusion?
------------- | --------------------------------| -------------------- | ----------------------------------| -----------------------
Randomization | ${y_1, y_2, \dots, y_N}$        | Treatment assignment | Hypothetical experiments     | ATE $\neq 0$?
Frequentist   | $Y$, $\boldsymbol{\theta}$, $N$ | Sample inclusion     | Confidence interval catches | Hypothesis test 
Supervised learning    | ${y_1, y_2, \dots, y_N}$        | Training / testing inclusion   | Loss in the testing data | Some procedure predicts best
Bayesian      | ${y_1, y_2, \dots, y_N}$, $\boldsymbol{\theta}$ | Beliefs about $\boldsymbol{\theta}$ | Functions of posterior draws of $\theta \mid y_1, y_2, \dots, y_N$ | Decision or action

## (Dis)Advantages of Bayesian Inference

- Bayesian inference remains useful in situations other paradigms specialize in:
    - Experiments: What are your beliefs about the ATE after seeing the data?
    - Repeated designs: Bayesian estimates have correct frequentist properties
    - Predictive modeling: If you only care about predictions, use the 
      posterior predictive distribution
- Bayesian inference is very useful when you are using the results to make a decision
  or take an action; other paradigms are not
- Bayesian inference is orders of magnitude more difficult for your computer because
  it is attempting to answer a more ambitious question
- The Bayesian approach is better suited for convincing yourself of something than
  convincing other people

## Objectivity and Subjectivity

- Under weak and not particularly controversial assumptions, Bayesian inference is THE objective way
  to update your beliefs about (functions of) $\theta$ in light of new data $y_1, y_2, \dots, y_N$
- Nevertheless, the Bayesian approach is labeled subjective because it does not say what your beliefs about 
  $\theta$ should be before you receive $y_1, y_2, \dots, y_N$
- Thus, if you currently believe something absurd about $\theta$ now, your beliefs about $\theta$ will
  merely be less absurd after updating them with $y_1, y_2, \dots, y_N$
- The big problem is not that people believe wrong things now, but that they do not update their 
  beliefs about $\theta$ according to Bayesian principles when they observe $y_1, y_2, \dots, y_N$
- In fact, in some situations, observing data that contradicts people's previous beliefs makes them
  believe in their wrong beliefs more strongly
